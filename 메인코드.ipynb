{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 분석 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모듈 불러오기\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from langdetect import detect_langs\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from soyclustering import SphericalKMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "from sklearn.svm import SVR\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "# 경로 설정\n",
    "os.chdir(r\"C:\\Users\\GilseungAhn\\Desktop\\한양대\\3. 프로젝트\\2. 진행중인 과제\\포스트코로나 AI 챌린지\")\n",
    "\n",
    "# 경고 무시하기\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 데이터 시간 범위 (date_range) 설정: 해외유입확진자 수 데이터가 정상수집된 2020년 4월 13일 이후 데이터만 필터링\n",
    "date_range = pd.date_range(start = pd.Timestamp(year = 2020, month = 4, day = 13),\n",
    "                           end = pd.Timestamp(year = 2020, month = 5, day = 5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 가공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 일자별 누적 해외유입 확진자 수 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.1 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_inflow_df = pd.read_csv(\"데이터/2. 외부데이터/일자별_누적해외유입확진자수.csv\", encoding = \"utf8\")\n",
    "corona_inflow_df['일자'] = pd.to_datetime(corona_inflow_df['일자']) # 데이터 타입을 날짜로 변환 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.2 데이터 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_inflow_df = corona_inflow_df[corona_inflow_df['일자'].between(date_range[0], date_range[-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.3 신규확진자 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i일의 신규확진자 = i일의 누적 확진자 - (i-1)일의 누적 확진자\n",
    "corona_inflow_df['신규확진자'] = corona_inflow_df['누적확진자'] - corona_inflow_df['누적확진자'].shift(1)\n",
    "\n",
    "# 첫 행에서 발생하는 결측 제거 \n",
    "corona_inflow_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1.4 데이터 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_inflow_df.to_csv(\"데이터/3. 정제데이터/일자별_신규해외유입확진자수.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 해외증시 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.1 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_stock_df = pd.read_csv(\"데이터/2. 외부데이터/일자별_해외지수.csv\", encoding = \"utf8\")\n",
    "\n",
    "# 데이터 타입을 날짜로 변환\n",
    "world_stock_df['일자'] = pd.to_datetime(world_stock_df['일자']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2 해외지수별 변동률 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i일 해외지수 변동률 = (i일 해외 지수 - (i-1)일 해외 지수) / (i-1)일 해외 지수 * 100\n",
    "world_stock_df['니케이지수_변동률'] = (world_stock_df['니케이지수'] - world_stock_df['니케이지수'].shift(1)) / world_stock_df['니케이지수'].shift(1) * 100\n",
    "world_stock_df['다우산업지수_변동률'] = (world_stock_df['다우산업지수'] - world_stock_df['다우산업지수'].shift(1)) / world_stock_df['다우산업지수'].shift(1) * 100\n",
    "world_stock_df['S&P500지수_변동률'] = (world_stock_df['S&P500지수'] - world_stock_df['S&P500지수'].shift(1)) / world_stock_df['S&P500지수'].shift(1) * 100\n",
    "world_stock_df['상해종합지수_변동률'] = (world_stock_df['상해종합지수'] - world_stock_df['상해종합지수'].shift(1)) / world_stock_df['상해종합지수'].shift(1) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.3 장이 열리지 않은 날의 지수를 이전 값으로 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일자를 인덱스로 정의한 뒤 date_range에 포함되지 않은 일자 추가 및 값을 이전값으로 대체\n",
    "world_date_range = pd.date_range(start = pd.Timestamp(year = 2020, month = 3, day = 1),\n",
    "                           end = pd.Timestamp(year = 2020, month = 5, day = 5)) \n",
    "\n",
    "world_stock_df = world_stock_df.set_index('일자').reindex(world_date_range).fillna(method = 'ffill').reset_index()\n",
    "\n",
    "# 일자를 인덱스로 정의하는 과정에서 생긴 index 컬럼을 일자 컬럼으로 재변환\n",
    "world_stock_df.rename(columns = {'index':'일자'}, inplace = True)\n",
    "world_stock_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.4 데이터 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_stock_df.to_csv(\"데이터/3. 정제데이터/일자별_해외지수_변동률포함.csv\", encoding = \"utf8\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 뉴스 기사 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.1 뉴스 목록 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4월 6일, 4월 29일, 5월 6일에 배포한 NewsList 불러오기\n",
    "NewsList_0406 = pd.read_excel(\"데이터/1. 제공데이터/0406_배포/3-1. NewsList.xls\")\n",
    "NewsList_0429 = pd.read_excel(\"데이터/1. 제공데이터/0429_배포/3-1. NewList.xls\")\n",
    "NewsList_0506 = pd.read_excel(\"데이터/1. 제공데이터/0506_배포/3-1. NewsList.xls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.2 뉴스 목록 데이터 병합 및 필터링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4월 6일, 4월 29일, 5월 6일에 배포한 NewsList 행 단위 병합\n",
    "NewsList = pd.concat([NewsList_0406, NewsList_0429, NewsList_0506], axis = 0, ignore_index = True)\n",
    "\n",
    "del NewsList_0406, NewsList_0429, NewsList_0506 # 메모리 관리를 위한 데이터 삭제\n",
    "\n",
    "# 게시일자를 날짜 타입으로 변환\n",
    "NewsList['게시일자'] = pd.to_datetime(NewsList['게시일자']).dt.date\n",
    "\n",
    "# 필터링 수행: 시간 기준, 감염병명 기준, 컬럼 기준\n",
    "NewsList = NewsList[NewsList['게시일자'].between(date_range[0], date_range[-1])] # 분석 범위에 해당하는 데이터만 필터링\n",
    "NewsList = NewsList[NewsList['감염병명'] == 'COVID-19'] # COVID-19 관련 기사만 가져오기\n",
    "NewsList = NewsList[['파일명', '제목', '게시일자']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.3 뉴스 기사 데이터 전처리 및 병합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() # 스테머 초기화\n",
    "\n",
    "# load_news_files: 파일명에 해당하는 파일의 내용을 불러오고 스테밍을 수행하는 함수\n",
    "def load_news_files(file):\n",
    "    try:\n",
    "        file_content = open(\"데이터/1. 제공데이터/0406_배포/3-2. Contents/\" + file + \".txt\", encoding = \"utf-8\").read()\n",
    "    except:\n",
    "        try:\n",
    "            file_content = open(\"데이터/1. 제공데이터/0429_배포/3-2. Contents/\" + file + \".txt\", encoding = \"utf-8\").read()\n",
    "        except:\n",
    "            file_content = open(\"데이터/1. 제공데이터/0506_배포/3-2. Contents/\" + file + \".txt\", encoding = \"utf-8\").read()\n",
    "    \n",
    "    file_content = file_content.split('Title :')[1].lower().strip()\n",
    "    output = ' '.join(list(set([stemmer.stem(w) for w in word_tokenize(file_content) if len(w) >= 2])))\n",
    "    return output\n",
    "\n",
    "NewsList['기사내용'] = NewsList['파일명'].apply(load_news_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기사 내용을 단어 가방으로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영어단어 목록 가져오기\n",
    "english_word_list = pd.Series(list(set(nltk.corpus.words.words()) - set(stopwords.words('english')))) # 불용어 제거\n",
    "english_word_list = english_word_list.apply(stemmer.stem).str.lower().drop_duplicates()\n",
    "english_word_list = english_word_list[english_word_list.apply(len) >= 2] # 길이가 2이상인 단어만 사용\n",
    "\n",
    "# 최소 하루에 한 번 이상 사용된 단어만 빈발 단어라고 간주\n",
    "def check_frequent_english_word(_word, _NewsList_per_day): \n",
    "    output = True\n",
    "    for news in _NewsList_per_day:\n",
    "        if _word not in news:\n",
    "            output = False\n",
    "            break\n",
    "    \n",
    "    return output\n",
    "\n",
    "NewsList_per_day = NewsList.groupby('게시일자')['기사내용'].sum().tolist()\n",
    "frequent_english_word_list = set(english_word_list[english_word_list.apply(check_frequent_english_word, _NewsList_per_day = NewsList_per_day)])\n",
    "\n",
    "# preprocessing_article(file): file을 읽어와서 텍스트 내에 있는 영어 단어만 추출하여 전처리한 결과를 반환하는 함수\n",
    "def preprocessing_article(Article, _frequent_english_word_list):\n",
    "    # 등장한 단어의 원형 집합 만들기\n",
    "    occur_word_list = set([stemmer.stem(w) for w in word_tokenize(Article) if len(w) >= 2])\n",
    "    \n",
    "    # 등장한 단어 가운데 영어만 추출\n",
    "    occur_english_word_list = frequent_english_word_list & occur_word_list\n",
    "    return ' '.join(list(occur_english_word_list))\n",
    "\n",
    "NewsList['기사내용'] = NewsList['기사내용'].apply(preprocessing_article, _frequent_english_word_list = frequent_english_word_list)\n",
    "NewsList.dropna(inplace = True) # 기사내용이 없거나, 영어가 아닌 기사는 제거\n",
    "\n",
    "with open(\"분석 결과 및 모델/단어목록.pckl\", \"wb\") as f:\n",
    "    pickle.dump(frequent_english_word_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 - 문서 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer를 vectorizer로 인스턴스화 및 초기화\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 기사 내용을 단어 문서 행렬로 변환 (셀: 해당 문서에 해당 단어의 출현 빈도)\n",
    "X = vectorizer.fit_transform(NewsList['기사내용'])\n",
    "X = X.toarray()\n",
    "\n",
    "# 출현한 모든 단어 가져오기\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# 단어-문서 행렬의 데이터 타입 변환 (np.array -> pd.DataFrame)\n",
    "word_document_matrix = pd.DataFrame(X, columns = words)\n",
    "\n",
    "del X # 메모리 관리를 위한 데이터 삭제\n",
    "\n",
    "# 날짜 부착 및 데이터 내보내기\n",
    "word_document_matrix['게시날짜'] = NewsList['게시일자'].values\n",
    "word_document_matrix.to_csv(\"데이터/3. 정제데이터/단어-문서행렬.csv\", index = False, encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.4 일자별 뉴스기사 수 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 게시일자에 따른 뉴스기사수 계산\n",
    "num_news_df = pd.DataFrame(NewsList['게시일자'].value_counts()).sort_values(by = '게시일자').reset_index()\n",
    "\n",
    "# groupby에 의한 바뀐 컬럼명 재변환 수행\n",
    "num_news_df.rename(columns = {\"게시일자\":\"뉴스기사수\"}, inplace = True)\n",
    "num_news_df.rename(columns = {\"index\":\"게시일자\"}, inplace = True)\n",
    "\n",
    "# date_range에 있으나 게시일자에 없는 날짜의 뉴스기사 수는 0으로 채우기\n",
    "num_news_df = num_news_df.set_index('게시일자').reindex(date_range).fillna(0).reset_index().rename(columns = {\"index\":\"게시일자\"})\n",
    "\n",
    "# 일자별_뉴스기사수 내보내기\n",
    "num_news_df.to_csv(\"데이터/3. 정제데이터/일자별_뉴스기사수.csv\", encoding = \"utf8\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 로밍 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.1 데이터 불러오기 및 행단위 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roming_df_0406 = pd.read_csv(\"데이터/1. 제공데이터/0406_배포/2. Roaming_data.csv\")\n",
    "roming_df_0429 = pd.read_csv(\"데이터/1. 제공데이터/0429_배포/2. Roaming_data.csv\")\n",
    "roming_df_0506 = pd.read_excel(\"데이터/1. 제공데이터/0506_배포/2. Roaming_data.xlsx\")\n",
    "\n",
    "roming_df = pd.concat([roming_df_0406, roming_df_0429, roming_df_0506], axis = 0, ignore_index = True)\n",
    "\n",
    "del roming_df_0406, roming_df_0429, roming_df_0506 # 메모리 관리를 위한 데이터 삭제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.2 return 컬럼 타입 변경 str -> TimeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert_string_to_datetime: M월 D일 형태의 문자열(s)를 TimeStamp로 변환하는 함수\n",
    "def convert_string_to_datetime(s): \n",
    "    s = str(s)\n",
    "    year, month, day = int(s[:4]), int(s[4:6]), int(s[6:])\n",
    "    return pd.Timestamp(year = year, month = month, day = day)\n",
    "\n",
    "# return 컬럼에 convert_string_to_datetime 함수 적용\n",
    "roming_df['return'] = roming_df['return'].apply(convert_string_to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.3 일자 및 국가에 따른 출입자 수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return 컬럼과 iso 컬럼 기준 count 합계 계산\n",
    "roming_df = roming_df.groupby(['return', 'iso'])['count'].sum().reset_index()\n",
    "\n",
    "# 등장하지 않은 경우 0으로 채운 뒤, 인덱스 리셋\n",
    "roming_df = roming_df.pivot('return', 'iso', 'count').fillna(0)\n",
    "roming_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4.4 입국자 합계 계산 및 데이터 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roming_df['입국자_합계'] = roming_df.iloc[:, 1:].sum(axis = 1)\n",
    "roming_df.to_csv(\"데이터/3. 정제데이터/일자별_입국자.csv\", index = False, encoding = \"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 기초 데이터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 일자 포맷 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_stock_df['일자'] = pd.to_datetime(world_stock_df['일자'])\n",
    "corona_inflow_df['일자'] = pd.to_datetime(corona_inflow_df['일자'])\n",
    "roming_df['return'] = pd.to_datetime(roming_df['return'])\n",
    "num_news_df['게시일자'] = pd.to_datetime(num_news_df['게시일자'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 기초 데이터 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.1 base_df 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = corona_inflow_df[['일자', '신규확진자']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.2 요일 변수 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요일 목록 정의\n",
    "weekday_list = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "\n",
    "# 일자에 해당하는 요일 변수 추가 (예: 2020년 5월 6일 (수) => Wednesday 컬럼만 1, 나머지는 0)\n",
    "for i in range(len(weekday_list)):\n",
    "    weekday = weekday_list[i]\n",
    "    base_df[weekday] = (base_df['일자'].dt.weekday == i).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 타 데이터와 병합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.1 예측 기간 변수 및 병합을 위한 임시 키 생성\n",
    "- D: 현재 시점으로부터 예측 시점까지의 남은 길이 (D = 1, 2, ..., 14)\n",
    "- D+k_이전일자: 현재 시점으로부터 D+k일 이전의 일자 (k = 0, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D = 1인 레코드만 먼저 생성\n",
    "D = 1\n",
    "base_df['D'] = D\n",
    "\n",
    "base_df['D+0_이전일자'] = base_df['일자'] - pd.DateOffset(days = D)\n",
    "base_df['D+1_이전일자'] = base_df['일자'] - pd.DateOffset(days = D+1)\n",
    "base_df['D+2_이전일자'] = base_df['일자'] - pd.DateOffset(days = D+2)\n",
    "base_df['D+3_이전일자'] = base_df['일자'] - pd.DateOffset(days = D+3) \n",
    "\n",
    "N = len(base_df) # D = 1인 레코드의 길이만큼 D = 2, 3, ..., 14도 생성하기 위해 길이 저장\n",
    "\n",
    "# D = 2, 3, ..., 14인 레코드 생성\n",
    "for D in range(2, 15):\n",
    "    # D = 1인 레코드만 복제하여 copy_base_df로 저장\n",
    "    copy_base_df = base_df.iloc[:N].copy()\n",
    "    \n",
    "    # D에 해당하는 D 컬럼 및 D+k_이전일자 컬럼 생성\n",
    "    copy_base_df['D'] = D\n",
    "    copy_base_df['D+0_이전일자'] = base_df['일자'] - pd.DateOffset(days = D)\n",
    "    copy_base_df['D+1_이전일자'] = base_df['일자'] - pd.DateOffset(days = D+1)\n",
    "    copy_base_df['D+2_이전일자'] = base_df['일자'] - pd.DateOffset(days = D+2)\n",
    "    copy_base_df['D+3_이전일자'] = base_df['일자'] - pd.DateOffset(days = D+3) \n",
    "    \n",
    "    # base_df에 copy_base_df 병합\n",
    "    base_df = pd.concat([base_df, copy_base_df], axis = 0, ignore_index = True, sort = True)\n",
    "\n",
    "# 메모리 관리를 위한 데이터 삭제\n",
    "del copy_base_df\n",
    "\n",
    "base_df.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.2 D+0_이전일자, D+1_이전일자의 신규 및 누적 확진자 부착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_inflow_df.rename({\"일자\":\"코로나일자\"}, axis = 1, inplace = True)\n",
    "for d in range(2):\n",
    "    # 효과적인 병합을 위한 컬럼명 변경 \n",
    "    # 신규확진자 -> D+k_이전일자 신규확진자 or D+(k-1)_이전일자 신규확진자 -> D+k_이전일자 신규확진자\n",
    "    corona_inflow_df.rename({\"신규확진자\": \"D+\" + str(d) + \"_이전일자_신규확진자\",\n",
    "                            \"누적확진자\": \"D+\" + str(d) + \"_이전일자_누적확진자\",\n",
    "                             \n",
    "                            \"D+\" + str(d-1) + \"_이전일자_신규확진자\": \"D+\" + str(d) + \"_이전일자_신규확진자\",\n",
    "                            \"D+\" + str(d-1) + \"_이전일자_누적확진자\": \"D+\" + str(d) + \"_이전일자_누적확진자\"},\n",
    "                           \n",
    "                            inplace = True,\n",
    "                           axis = 1)\n",
    "    \n",
    "    base_df = pd.merge(base_df, corona_inflow_df, left_on = 'D+' + str(d) + \"_이전일자\", right_on = '코로나일자').drop('코로나일자', axis = 1)\n",
    "    \n",
    "# 메모리 관리를 위한 데이터 삭제\n",
    "del corona_inflow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.3 D+k_이전일자의 해외증시 부착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "world_stock_df.rename(columns = {'일자':'증시일자'}, inplace = True)\n",
    "for d in range(4):\n",
    "    base_df = pd.merge(base_df, world_stock_df, left_on = 'D+' + str(d) + \"_이전일자\", right_on = '증시일자').drop('증시일자', axis = 1)\n",
    "    for col in world_stock_df.columns:\n",
    "        base_df.rename(columns = {col: \"D+\" + str(d) + \"_이전일자_\" + col}, inplace = True)\n",
    "\n",
    "# 메모리 관리를 위한 데이터 삭제\n",
    "del world_stock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.4 D+k_이전일자의 입국자 부착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(4):\n",
    "    base_df = pd.merge(base_df, roming_df, left_on = 'D+' + str(d) + \"_이전일자\", right_on = 'return').drop('return', axis = 1)\n",
    "    for col in roming_df.columns:\n",
    "        base_df.rename(columns = {col: \"D+\" + str(d) + \"_이전일자_\" + col}, inplace = True)\n",
    "\n",
    "# 메모리 관리를 위한 데이터 삭제\n",
    "del roming_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.5 D+k_이전일자의 뉴스 개수 데이터 부착"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in range(4):\n",
    "    base_df = pd.merge(base_df, num_news_df, left_on = 'D+' + str(d) + \"_이전일자\", right_on = '게시일자').drop('게시일자', axis = 1)\n",
    "    for col in num_news_df.columns:\n",
    "        base_df.rename(columns = {col: \"D+\" + str(d) + \"_이전일자_\" + col}, inplace = True)\n",
    "\n",
    "# 메모리 관리를 위한 데이터 삭제\n",
    "del num_news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3.6 기초 데이터 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df.to_csv(\"데이터/3. 정제데이터/기초데이터.csv\", encoding = \"utf8\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 단어군집 및 단어군집 - 문서 행렬 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_wcluster_document_df: 단어 - 문서 행렬을 단어 군집 - 문서 행렬로 변환하는 함수\n",
    "def make_wcluster_document_df(_word_document_matrix, num_cluster): # _ (prefix)는 기존 데이터와 함수 입력이 충돌하지 않도록 넣은 것    \n",
    "    # 일자 변수 저장\n",
    "    date_values = _word_document_matrix['게시날짜'].values   \n",
    "    _word_document_matrix.drop('게시날짜', axis = 1, inplace = True)\n",
    "    \n",
    "    # transpose를 사용하여 단어 - 문서 행렬을 문서 - 단어 행렬로 변환\n",
    "    document_word_matrix = _word_document_matrix.T\n",
    "    \n",
    "    # 군집화 수행 및 군집 결과 사전화 (key: 단어, value: 군집 번호)\n",
    "    spherical_kmeans = SphericalKMeans(n_clusters = num_cluster, max_iter = 1000, verbose = 0, init = 'similar_cut')    \n",
    "    word_cluster_label = spherical_kmeans.fit_predict(csr_matrix(document_word_matrix))\n",
    "    word_cluster_dict = pd.Series(word_cluster_label, index = list(_word_document_matrix.columns)).apply(lambda x:\"단어군집_\" + str(x+1)).to_dict()    \n",
    "    \n",
    "    # 단어로 된 컬럼 이름을 군집 번호로 바꾸기\n",
    "    _word_document_matrix.rename(columns = word_cluster_dict, inplace = True)\n",
    "    \n",
    "    # 각 군집에 속한 단어 등장 횟수 합계 계산 \n",
    "    wcluster_document_df = pd.DataFrame()\n",
    "    for x in range(1, num_cluster + 1):\n",
    "        try:\n",
    "            column_values = _word_document_matrix[\"단어군집_\" + str(x)].sum(axis = 1).values\n",
    "        except:\n",
    "            column_values = _word_document_matrix[\"단어군집_\" + str(x)]\n",
    "        wcluster_document_df[\"단어군집_\" + str(x)] = column_values\n",
    "        \n",
    "    # 기초 데이터와 병합을 위한 게시 날짜 변수 추가\n",
    "    wcluster_document_df['게시날짜'] = date_values\n",
    "    \n",
    "    # 날짜별 단어_군집 변수 합계 계산\n",
    "    cluster_columns = [\"단어군집_\" + str(x) for x in range(1, num_cluster + 1)]\n",
    "    wcluster_document_df = wcluster_document_df.groupby('게시날짜', as_index = False)[cluster_columns].sum()\n",
    "    \n",
    "    # 각 군집에 속한 단어 등장 비율 계산 \n",
    "    wcluster_document_df[cluster_columns] = wcluster_document_df[cluster_columns].values / wcluster_document_df[cluster_columns].sum(axis = 1).values.reshape(len(wcluster_document_df), 1)\n",
    "        \n",
    "    return word_cluster_dict, wcluster_document_df\n",
    "\n",
    "# 단어군집 및 단어군집 - 문서 행렬 생성 \n",
    "word_document_matrix = pd.read_csv(\"데이터/3. 정제데이터/단어-문서행렬.csv\", encoding = \"utf8\")\n",
    "base_df = pd.read_csv(\"데이터/3. 정제데이터/기초데이터.csv\", encoding = \"utf8\")\n",
    "num_cluster = 40\n",
    "word_cluster_dict, wcluster_document_df = make_wcluster_document_df(word_document_matrix, num_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(_wcluster_document_df, _base_df, K): \n",
    "    _wcluster_document_df['게시날짜'] = pd.to_datetime(_wcluster_document_df['게시날짜'])\n",
    "    for d in range(4):\n",
    "        _base_df[\"D+\" + str(d) + \"_이전일자\"] = pd.to_datetime(_base_df[\"D+\" + str(d) + \"_이전일자\"])\n",
    "        _base_df = pd.merge(_base_df, _wcluster_document_df, left_on = \"D+\" + str(d) + \"_이전일자\", right_on = '게시날짜').drop('게시날짜', axis = 1)\n",
    "        for col in _wcluster_document_df.columns:\n",
    "            _base_df.rename(columns = {col:\"D+\" + str(d) + \"_이전일자_\" + col}, inplace = True)\n",
    "\n",
    "    _base_df.sort_values(by = '일자', inplace = True)\n",
    "    day_list = _base_df['일자'].unique()\n",
    "    \n",
    "    train_df = _base_df\n",
    "    \n",
    "    sample_weight = np.array(np.exp(K * np.arange(len(day_list)) / len(day_list)), dtype = int) - 1    \n",
    "    sample_weight_dict = pd.Series(sample_weight, index = day_list).to_dict()\n",
    "    for day, weight in sample_weight_dict.items():\n",
    "        if weight > 0:\n",
    "            duplicated_samples = train_df[train_df['일자'] == day]\n",
    "            train_df = train_df.append([duplicated_samples] * weight, ignore_index = True)\n",
    "\n",
    "    X = train_df.drop(['신규확진자', '일자', 'D+0_이전일자', 'D+1_이전일자', 'D+2_이전일자', 'D+3_이전일자'], axis = 1)       \n",
    "    Y = train_df['신규확진자']    \n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "K = 2\n",
    "X, Y = generate_training_data(wcluster_document_df, base_df, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 특징 선택 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 300\n",
    "feature_selector = SelectKBest(f_regression, k = k).fit(X, Y)\n",
    "selected_features = list(X.columns[feature_selector.get_support()])\n",
    "selected_features = list(set(selected_features + ['D', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 최종 모델 정의 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVR(C = 4, degree = 1, epsilon = 1, gamma = 0.1, kernel = 'rbf', max_iter = 10000)\n",
    "model.fit(X[selected_features], Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 학습 결과 내보내기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종 군집, 특징 집합, 모델 내보내기\n",
    "with open(\"분석 결과 및 모델/최종_군집.pckl\", \"wb\") as f:\n",
    "    pickle.dump(word_cluster_dict, f)\n",
    "\n",
    "with open(\"분석 결과 및 모델/최종_특징집합.pckl\", \"wb\") as f:\n",
    "    pickle.dump(selected_features, f)\n",
    "    \n",
    "with open(\"분석 결과 및 모델/최종_모델.pckl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 예측 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 예측 시점 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5월 2일 ~ 5일 데이터를 활용하여, 5월 6일 이후 2주 예측\n",
    "input_date_range = pd.date_range(start = pd.Timestamp(year = 2020, month = 5, day = 2),\n",
    "                           end = pd.Timestamp(year = 2020, month = 5, day = 5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 데이터 불러오기 및 필터링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.1 누적해외유입확진자 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "corona_inflow_df = pd.read_csv(\"데이터/2. 외부데이터/일자별_누적해외유입확진자수.csv\", encoding = \"utf8\")\n",
    "\n",
    "# 데이터 타입을 날짜로 변환\n",
    "corona_inflow_df['일자'] = pd.to_datetime(corona_inflow_df['일자'])  \n",
    "\n",
    "# 신규확진자 계산\n",
    "corona_inflow_df['신규확진자'] = corona_inflow_df['누적확진자'] - corona_inflow_df['누적확진자'].shift(1)\n",
    "\n",
    "# 기간 필터링\n",
    "corona_inflow_df = corona_inflow_df[corona_inflow_df['일자'].between(input_date_range[0], input_date_range[-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.2 해외 증시 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "world_stock_df = pd.read_csv(\"데이터/2. 외부데이터/일자별_해외지수.csv\", encoding = \"utf8\")\n",
    "world_stock_df['일자'] = pd.to_datetime(world_stock_df['일자']) # 데이터 타입을 날짜로 변환 \n",
    "\n",
    "# 변동률 계산\n",
    "world_stock_df['니케이지수_변동률'] = (world_stock_df['니케이지수'] - world_stock_df['니케이지수'].shift(1)) / world_stock_df['니케이지수'].shift(1) * 100\n",
    "world_stock_df['다우산업지수_변동률'] = (world_stock_df['다우산업지수'] - world_stock_df['다우산업지수'].shift(1)) / world_stock_df['다우산업지수'].shift(1) * 100\n",
    "world_stock_df['S&P500지수_변동률'] = (world_stock_df['S&P500지수'] - world_stock_df['S&P500지수'].shift(1)) / world_stock_df['S&P500지수'].shift(1) * 100\n",
    "world_stock_df['상해종합지수_변동률'] = (world_stock_df['상해종합지수'] - world_stock_df['상해종합지수'].shift(1)) / world_stock_df['상해종합지수'].shift(1) * 100\n",
    "\n",
    "# Null값 채우기\n",
    "date_range = pd.date_range(start= world_stock_df['일자'].min(), end = world_stock_df['일자'].max())\n",
    "world_stock_df = world_stock_df.set_index('일자').reindex(date_range).fillna(method = 'ffill').reset_index()\n",
    "world_stock_df.rename(columns = {'index':'일자'}, inplace = True)\n",
    "\n",
    "# 기간 필터링\n",
    "world_stock_df = world_stock_df[world_stock_df['일자'].between(input_date_range[0], input_date_range[-1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.3 뉴스 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_wcluster_document_df_based_on_previous_cluster_model: 이전 군집 결과를 바탕으로 단어 - 군집 행렬을 생성하는 함수\n",
    "def make_wcluster_document_df_based_on_previous_cluster_model(_word_document_matrix, word_cluster_dict): # _ (prefix)는 기존 데이터와 함수 입력이 충돌하지 않도록 넣은 것    \n",
    "    # 일자 변수 저장\n",
    "    date_values = _word_document_matrix['게시날짜'].values          \n",
    "\n",
    "    # 단어로 된 컬럼 이름을 군집 번호로 바꾸기\n",
    "    _word_document_matrix.rename(columns = word_cluster_dict, inplace = True)\n",
    "    \n",
    "    # 각 군집에 속한 단어 등장 횟수 합계 계산 \n",
    "    wcluster_document_df = pd.DataFrame()\n",
    "    num_cluster = len(set(word_cluster_dict.values()))\n",
    "    for x in range(1, num_cluster + 1):\n",
    "        try:\n",
    "            column_values = _word_document_matrix[\"단어군집_\" + str(x)].sum(axis = 1).values\n",
    "        except:\n",
    "            column_values = _word_document_matrix[\"단어군집_\" + str(x)]\n",
    "        wcluster_document_df[\"단어군집_\" + str(x)] = column_values\n",
    "            \n",
    "    # 기초 데이터와 병합을 위한 게시 날짜 변수 추가\n",
    "    wcluster_document_df['게시날짜'] = date_values\n",
    "    \n",
    "    # 날짜별 단어_군집 변수 합계 계산\n",
    "    cluster_columns = [\"단어군집_\" + str(x) for x in range(1, num_cluster + 1)]\n",
    "    wcluster_document_df = wcluster_document_df.groupby('게시날짜', as_index = False)[cluster_columns].sum()\n",
    "    \n",
    "    # 각 군집에 속한 단어 등장 비율 계산 \n",
    "    wcluster_document_df[cluster_columns] = wcluster_document_df[cluster_columns].values / wcluster_document_df[cluster_columns].sum(axis = 1).values.reshape(len(wcluster_document_df), 1)\n",
    "        \n",
    "    return wcluster_document_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "NewsList = pd.read_excel(\"데이터/1. 제공데이터/0506_배포/3-1. NewsList.xls\")\n",
    "\n",
    "# 데이터 타입을 날짜로 변환 (시간 제외)\n",
    "NewsList['게시일자'] = pd.to_datetime(NewsList['게시일자']).dt.date\n",
    "\n",
    "# COVID-19 관련 기사만 가져오기\n",
    "NewsList = NewsList[NewsList['감염병명'] == 'COVID-19'] \n",
    "\n",
    "# 기간 및 컬럼 필터링\n",
    "NewsList = NewsList[['파일명', '제목', '게시일자']]\n",
    "NewsList = NewsList[NewsList['게시일자'].between(input_date_range[0], input_date_range[-1])]\n",
    "\n",
    "# 뉴스기사 병합\n",
    "NewsList['기사내용'] = NewsList['파일명'].apply(load_news_files)\n",
    "NewsList.dropna(inplace = True)\n",
    "\n",
    "# 뉴스기사 전처리\n",
    "# 단어 목록 가져오기\n",
    "with open(\"분석 결과 및 모델/단어목록.pckl\", \"rb\") as f:\n",
    "    frequent_english_word_list = pickle.load(f)\n",
    "\n",
    "NewsList['기사내용'] = NewsList['기사내용'].apply(preprocessing_article, _frequent_english_word_list = frequent_english_word_list)\n",
    "\n",
    "# 단어 - 문서 행렬로 변환\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(NewsList['기사내용'])\n",
    "X = X.toarray()\n",
    "words = vectorizer.get_feature_names()\n",
    "word_document_matrix = pd.DataFrame(X, columns = words)\n",
    "\n",
    "# 최종 군집 사전 가져오기\n",
    "with open(\"분석 결과 및 모델/최종_군집.pckl\", \"rb\") as f:\n",
    "    word_cluster_dict = pickle.load(f)\n",
    "    \n",
    "# 단어 목록 가져오기\n",
    "model_word_list = list(word_cluster_dict.keys())    \n",
    "\n",
    "model_word_list = list(set(model_word_list) & set(word_document_matrix.columns))   \n",
    "word_document_matrix = word_document_matrix[model_word_list]\n",
    "\n",
    "# 게시 날짜 부착\n",
    "word_document_matrix['게시날짜'] = NewsList['게시일자'].values\n",
    "\n",
    "# 단어 군집 - 문서 행렬로 변환\n",
    "wcluster_document_df = make_wcluster_document_df_based_on_previous_cluster_model(word_document_matrix, word_cluster_dict)\n",
    "wcluster_document_df['게시날짜'] = pd.to_datetime(wcluster_document_df['게시날짜'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.4 로밍 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roming_df = pd.read_excel(\"데이터/1. 제공데이터/0506_배포/2. Roaming_data.xlsx\")\n",
    "roming_df['return'] = roming_df['return'].apply(convert_string_to_datetime)\n",
    "roming_df = roming_df.groupby(['return', 'iso'])['count'].sum().reset_index()\n",
    "roming_df = roming_df.pivot('return', 'iso', 'count').fillna(0)\n",
    "roming_df.reset_index(inplace = True)\n",
    "roming_df['입국자_합계'] = roming_df.iloc[:, 1:].sum(axis = 1)\n",
    "\n",
    "# 필터링 수행\n",
    "# training_roming_df_cols: 학습 로밍 데이터에 포함된 국가 목록\n",
    "training_roming_df_cols = open(\"데이터/3. 정제데이터/일자별_입국자.csv\", encoding = \"utf8\").readline().split(',')\n",
    "training_roming_df_cols[-1] = training_roming_df_cols[-1][:-1]\n",
    "\n",
    "roming_df = roming_df[roming_df['return'].between(input_date_range[0], input_date_range[-1])]\n",
    "\n",
    "for col in training_roming_df_cols:\n",
    "    if col not in roming_df.columns:\n",
    "        roming_df[col] = 0\n",
    "\n",
    "roming_df = roming_df[training_roming_df_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2.5 뉴스기사 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_num_News = pd.DataFrame(NewsList['게시일자'].value_counts()).sort_values(by = '게시일자').reset_index()\n",
    "day_num_News.rename(columns = {\"게시일자\":\"뉴스기사수\"}, inplace = True)\n",
    "day_num_News.rename(columns = {\"index\":\"게시일자\"}, inplace = True)\n",
    "day_num_News = day_num_News.set_index('게시일자').reindex(input_date_range).fillna(0).reset_index().rename(columns = {\"index\":\"게시일자\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 학습 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 초기화\n",
    "prediction_time = []\n",
    "D = []\n",
    "for i in range(1, 15):\n",
    "    prediction_time.append(input_date_range[-1] + pd.Timedelta(i, unit = 'd'))\n",
    "    D.append(i)\n",
    "\n",
    "train_df = pd.DataFrame({\"일자\":prediction_time, \"D\": D})\n",
    "\n",
    "# 요일 변수 추가\n",
    "weekday_list = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "for i in range(len(weekday_list)):\n",
    "    weekday = weekday_list[i]\n",
    "    train_df[weekday] = (train_df['일자'].dt.weekday == i).astype(int)\n",
    "\n",
    "# 예측 기간 변수 추가  \n",
    "train_df['D+0_이전일자'] = input_date_range[-1]\n",
    "train_df['D+1_이전일자'] = input_date_range[-1] - pd.DateOffset(days = 1)\n",
    "train_df['D+2_이전일자'] = input_date_range[-1] - pd.DateOffset(days = 2)\n",
    "train_df['D+3_이전일자'] = input_date_range[-1] - pd.DateOffset(days = 3) \n",
    "\n",
    "# D-0, D-1 시점의 신규, 누적 확진자 부착\n",
    "for d in range(2):\n",
    "    date = train_df['D+' + str(d) + '_이전일자'].iloc[0]\n",
    "    train_df['D+' + str(d) + '_이전일자_신규확진자'] = corona_inflow_df[corona_inflow_df['일자'] == date].iloc[0, 2]\n",
    "    train_df['D+' + str(d) + '_이전일자_누적확진자'] = corona_inflow_df[corona_inflow_df['일자'] == date].iloc[0, 1]\n",
    "\n",
    "# D-0, D-1, D-2, D-3 시점의 해외증시, 입국자, 뉴스 국가 출현 데이터 부착\n",
    "for d in range(4):\n",
    "    date = train_df['D+' + str(d) + '_이전일자'].iloc[0]    \n",
    "    for c in range(1, len(world_stock_df.columns)):\n",
    "        train_df['D+' + str(d) + '_이전일자_' + world_stock_df.columns[c]] = world_stock_df[world_stock_df['일자'] == date].iloc[0, c]\n",
    "    \n",
    "    for c in range(1, len(roming_df.columns)):\n",
    "        train_df['D+' + str(d) + '_이전일자_' + roming_df.columns[c]] = roming_df[roming_df['return'] == date].iloc[0, c]\n",
    "\n",
    "    for c in range(1, len(wcluster_document_df.columns)):\n",
    "        train_df['D+' + str(d) + '_이전일자_' + wcluster_document_df.columns[c]] = wcluster_document_df[wcluster_document_df['게시날짜'] == date].iloc[0, c]\n",
    "        \n",
    "    train_df['D+' + str(d) + '_이전일자_' + '뉴스기사수'] = day_num_News[day_num_News['게시일자'] == date].iloc[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 모델 불러오기 및 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.1 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"분석 결과 및 모델/최종_특징집합.pckl\", \"rb\") as f:\n",
    "    selected_features = pickle.load(f)\n",
    "\n",
    "with open(\"분석 결과 및 모델/최종_모델.pckl\", \"rb\") as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4.2 예측 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.copy()\n",
    "X = X.drop(['일자', 'D+0_이전일자', 'D+1_이전일자', 'D+2_이전일자', 'D+3_이전일자'], axis = 1)\n",
    "\n",
    "X = X[selected_features]\n",
    "Y = model.predict(X)\n",
    "Y[Y < 0] = 0\n",
    "Y = np.round(Y, 0).astype(int)\n",
    "#Y = np.array(Y, dtype = int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 예측 결과 보여주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(Y, index = train_df['일자'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 분석 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from soyclustering import SphericalKMeans\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.svm import SVR\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "os.chdir(r\"C:\\Users\\GilseungAhn\\Desktop\\한양대\\3. 프로젝트\\2. 진행중인 과제\\포스트코로나 AI 챌린지\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 필요 함수 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### make_wcluster_document_df: 단어군집 - 문서 행렬 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_wcluster_document_df: 단어 - 문서 행렬을 단어 군집 - 문서 행렬로 변환하는 함수\n",
    "def make_wcluster_document_df(_word_document_matrix, num_cluster, date_values): # _ (prefix)는 기존 데이터와 함수 입력이 충돌하지 않도록 넣은 것    \n",
    "    # 일자 변수 저장\n",
    "    #date_values = _word_document_matrix['게시날짜'].values  \n",
    "    \n",
    "    if '게시날짜' in _word_document_matrix.columns:\n",
    "        _word_document_matrix.drop('게시날짜', axis = 1, inplace = True)\n",
    "    \n",
    "    #_word_document_matrix.drop('게시날짜', axis = 1, inplace = True)\n",
    "    \n",
    "    # transpose를 사용하여 단어 - 문서 행렬을 문서 - 단어 행렬로 변환\n",
    "    document_word_matrix = _word_document_matrix.T\n",
    "    \n",
    "    # 군집화 수행 및 군집 결과 사전화 (key: 단어, value: 군집 번호)\n",
    "    spherical_kmeans = SphericalKMeans(n_clusters = num_cluster, max_iter = 1000, verbose = 0, init = 'similar_cut')    \n",
    "    word_cluster_label = spherical_kmeans.fit_predict(csr_matrix(document_word_matrix))\n",
    "    word_cluster_dict = pd.Series(word_cluster_label, index = list(_word_document_matrix.columns)).apply(lambda x:\"단어군집_\" + str(x+1)).to_dict()    \n",
    "    \n",
    "    # 단어로 된 컬럼 이름을 군집 번호로 바꾸기\n",
    "    _word_document_matrix.rename(columns = word_cluster_dict, inplace = True)\n",
    "    \n",
    "    # 각 군집에 속한 단어 등장 횟수 합계 계산 \n",
    "    wcluster_document_df = pd.DataFrame()\n",
    "    for x in range(1, num_cluster + 1):\n",
    "        try:\n",
    "            column_values = _word_document_matrix[\"단어군집_\" + str(x)].sum(axis = 1).values\n",
    "        except:\n",
    "            column_values = _word_document_matrix[\"단어군집_\" + str(x)]\n",
    "        wcluster_document_df[\"단어군집_\" + str(x)] = column_values\n",
    "        \n",
    "    # 기초 데이터와 병합을 위한 게시 날짜 변수 추가\n",
    "    wcluster_document_df['게시날짜'] = date_values\n",
    "    \n",
    "    # 날짜별 단어_군집 변수 합계 계산\n",
    "    cluster_columns = [\"단어군집_\" + str(x) for x in range(1, num_cluster + 1)]\n",
    "    wcluster_document_df = wcluster_document_df.groupby('게시날짜', as_index = False)[cluster_columns].sum()\n",
    "    \n",
    "    # 각 군집에 속한 단어 등장 비율 계산 \n",
    "    wcluster_document_df[cluster_columns] = wcluster_document_df[cluster_columns].values / wcluster_document_df[cluster_columns].sum(axis = 1).values.reshape(len(wcluster_document_df), 1)\n",
    "        \n",
    "    return word_cluster_dict, wcluster_document_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습 데이터 생성:  _wcluster_document_df와 _base_df 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(_wcluster_document_df, _base_df, K): \n",
    "    _wcluster_document_df['게시날짜'] = pd.to_datetime(_wcluster_document_df['게시날짜'])    \n",
    "    for d in range(4):\n",
    "        _base_df[\"D+\" + str(d) + \"_이전일자\"] = pd.to_datetime(_base_df[\"D+\" + str(d) + \"_이전일자\"])\n",
    "        _base_df = pd.merge(_base_df, _wcluster_document_df, left_on = \"D+\" + str(d) + \"_이전일자\", right_on = '게시날짜').drop('게시날짜', axis = 1)\n",
    "        for col in _wcluster_document_df.columns:\n",
    "            _base_df.rename(columns = {col:\"D+\" + str(d) + \"_이전일자_\" + col}, inplace = True)\n",
    "\n",
    "    _base_df.sort_values(by = '일자', inplace = True)\n",
    "    day_list = _base_df['일자'].unique()\n",
    "\n",
    "    # 일자 기준 데이터 분리 (2주까지 데이터가 예측)\n",
    "    train_index = _base_df['일자'] < day_list[-14]\n",
    "    test_index = _base_df['일자'] >= day_list[-14]\n",
    "\n",
    "    train_df = _base_df.loc[train_index]\n",
    "    test_df = _base_df.loc[test_index]\n",
    "    \n",
    "    # 최근 데이터 재샘플링 수행\n",
    "    if K > 0:\n",
    "        sample_weight = np.array(np.exp(K * np.arange(len(day_list)) / len(day_list)), dtype = int) - 1    \n",
    "        sample_weight_dict = pd.Series(sample_weight, index = day_list).to_dict()\n",
    "        for day, weight in sample_weight_dict.items():\n",
    "            if weight > 0:\n",
    "                duplicated_samples = train_df[train_df['일자'] == day]\n",
    "                train_df = train_df.append([duplicated_samples] * weight, ignore_index = True)\n",
    "\n",
    "    # 특징 벡터와 라벨 분리\n",
    "    Train_X = train_df.drop(['신규확진자', '일자', 'D+0_이전일자', 'D+1_이전일자', 'D+2_이전일자', 'D+3_이전일자'], axis = 1)    \n",
    "    Test_X = test_df.drop(['신규확진자', '일자', 'D+0_이전일자', 'D+1_이전일자', 'D+2_이전일자', 'D+3_이전일자'], axis = 1)    \n",
    "    \n",
    "    Train_Y = train_df['신규확진자']    \n",
    "    Test_Y = test_df['신규확진자']\n",
    "    \n",
    "    return Train_X, Test_X, Train_Y, Test_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 특징 점수 기준 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_ranking(Train_X, Train_Y):    \n",
    "    output = pd.Series(f_regression(Train_X, Train_Y)[1], index = Train_X.columns).sort_values()\n",
    "    D_ind = output.index.tolist().index('D')\n",
    "    return pd.Index(['D'] + output.index.tolist()[:D_ind] + output.index.tolist()[D_ind+1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 목록 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_list():\n",
    "    model_list = []\n",
    "    model_parameter_dict = dict()\n",
    "    \n",
    "    model_parameter_dict[SVR] = ParameterGrid({\"epsilon\": [0.1, 0.5, 1, 2, 5, 10],\n",
    "                                              \"C\": [2**-6, 2**-5, 2**-4, 2**-3, 2**-2, 2**-1, 2**0, 2**1, 2**2, 2**3],\n",
    "                                              \"kernel\": ['linear', 'rbf', 'poly'],\n",
    "                                              \"gamma\": [1e-08, 1e-07, 1e-06, 1e-05, 1e-04, 1e-03, 1e-02, 1e-01, 1, 10, 100, 'scale', 'auto'],\n",
    "                                               \"degree\": [1, 2, 3, 4],\n",
    "                                              \"max_iter\": [50, 100, 1000, 10000, 100000]})\n",
    "                                                                  \n",
    "    model_name_list = []\n",
    "    model_parameter_list = []\n",
    "    \n",
    "    for model in model_parameter_dict.keys():\n",
    "        for parameter in model_parameter_dict[model]:   \n",
    "            if parameter['kernel'] == 'linear' and parameter['gamma'] != 1e-08:\n",
    "                pass\n",
    "            elif parameter['kernel'] != 'poly' and parameter['degree'] != 1:\n",
    "                pass\n",
    "\n",
    "            model_list.append(model(**parameter))\n",
    "            model_name_list.append(str(model).split('(')[0].split('.')[-1][:-2])\n",
    "            parameter_name = str(parameter).replace('\"', '')\n",
    "            parameter_name = parameter_name.replace(\"'\", '')\n",
    "            parameter_name = parameter_name.replace(\"{\", '')\n",
    "            parameter_name = parameter_name.replace(\"}\", '')\n",
    "\n",
    "            model_parameter_list.append(parameter_name)\n",
    "    \n",
    "    return model_list, model_name_list, model_parameter_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 평가 (제공)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(_model, _Text_X, _Test_Y):\n",
    "    pred_Y = _model.predict(_Text_X)\n",
    "    pred_Y[pred_Y < 0] = 0\n",
    "    pred_Y = np.round(pred_Y)\n",
    "    Num = sum((_Test_Y - pred_Y) ** 2)\n",
    "    Dem = sum(_Test_Y ** 2)\n",
    "    score = 100 * (1- Num / Dem)\n",
    "    return score, np.std(pred_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 튜닝 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 튜닝 범위 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_list = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 100]\n",
    "n_feature_list = np.arange(50, 501, 50)\n",
    "K_list = [0, 1, 2, 3, 4, 5]\n",
    "model_list, model_name_list, model_parameter_list = make_model_list()\n",
    "max_iteration = 100000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 튜닝 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이터레이션: 0/16848000, 최고점수: 81.27460629921259, 표준편차: 1.3941301822131522\n",
      "이터레이션: 195/16848000, 최고점수: 81.70521653543307, 표준편차: 1.3251984930593432\n",
      "이터레이션: 230/16848000, 최고점수: 82.19734251968505, 표준편차: 0.4729422800359504\n",
      "이터레이션: 245/16848000, 최고점수: 83.50147637795276, 표준편차: 0.3460225292082949\n",
      "이터레이션: 250/16848000, 최고점수: 84.00590551181102, 표준편차: 0.0\n",
      "이터레이션: 585/16848000, 최고점수: 85.00246062992126, 표준편차: 0.0\n",
      "이터레이션: 37940/16848000, 최고점수: 85.72834645669292, 표준편차: 0.33165520597480563\n"
     ]
    }
   ],
   "source": [
    "word_document_matrix = pd.read_csv(\"데이터/3. 정제데이터/단어-문서행렬.csv\", encoding = \"utf8\")    \n",
    "date_values = word_document_matrix['게시날짜'].values\n",
    "base_df = pd.read_csv(\"데이터/3. 정제데이터/기초데이터.csv\", encoding = \"utf8\")\n",
    "\n",
    "# 튜닝 결과 저장 파일 및 헤더 생성\n",
    "f = open(\"분석 결과 및 모델/파라미터튜닝결과.txt\", \"w\")\n",
    "f.write(\"num_cluster\\tK\\tk\\tC\\tdegree\\tepsilon\\tgamma\\tkernel\\tmax_iter\\tscore\\tscore_std\\n\")\n",
    "f.close()\n",
    "\n",
    "i = 0\n",
    "best_score = 0\n",
    "\n",
    "while i < max_iteration:\n",
    "    word_document_matrix = pd.read_csv(\"데이터/3. 정제데이터/단어-문서행렬.csv\", encoding = \"utf8\")    \n",
    "    date_values = word_document_matrix['게시날짜'].values\n",
    "    base_df = pd.read_csv(\"데이터/3. 정제데이터/기초데이터.csv\", encoding = \"utf8\")\n",
    "    \n",
    "    num_cluster = random.choice(n_clusters_list)\n",
    "    K = random.choice(K_list)\n",
    "    k = random.choice(n_feature_list)\n",
    "    _, wcluster_document_df = make_wcluster_document_df(word_document_matrix, num_cluster, date_values)\n",
    "    Train_X, Test_X, Train_Y, Test_Y = generate_training_data(wcluster_document_df, base_df, K)\n",
    "    feature_rank = feature_ranking(Train_X, Train_Y)        \n",
    "    selected_features = list(set(list(feature_rank[:k]) + ['D', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']))\n",
    "    feature_selected_Train_X = Train_X[selected_features]\n",
    "    feature_selected_Test_X = Test_X[selected_features]            \n",
    "\n",
    "    for (model, model_name, parameter_name) in zip(model_list, model_name_list, model_parameter_list):\n",
    "        model.fit(feature_selected_Train_X, Train_Y)\n",
    "        score, score_std = model_test(model, feature_selected_Test_X, Test_Y)   \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_parameter = parameter_name\n",
    "            best_K = K\n",
    "            f = open(\"분석 결과 및 모델/파라미터튜닝결과.txt\", \"a\")\n",
    "            f.write(str(num_cluster))\n",
    "            f.write('\\t')\n",
    "            f.write(str(K))\n",
    "            f.write('\\t')\n",
    "            f.write(str(k))\n",
    "            f.write('\\t')\n",
    "            f.write(best_parameter)\n",
    "            f.write('\\t')\n",
    "            f.write(str(score))\n",
    "            f.write('\\t')\n",
    "            f.write(str(score_std))\n",
    "            f.write('\\n')\n",
    "            f.close()\n",
    "            print(\"이터레이션: {}/{}, 최고점수: {}, 표준편차: {}\".format(i, max_iteration, best_score, score_std))\n",
    "\n",
    "        i += 1\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
